{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22bf8d0",
   "metadata": {},
   "source": [
    "## Classify Text into Labels\n",
    "As per this [tutorial](https://python.langchain.com/docs/tutorials/classification/)\n",
    "\n",
    "Tagging means labeling a document with classes such as:\n",
    "- Sentiment\n",
    "- Language\n",
    "- Style (formal, informal etc.)\n",
    "- Covered topics\n",
    "- Political tendency\n",
    "\n",
    "Tagging has a few components:\n",
    "\n",
    "- function: Like extraction, tagging uses functions to specify how the model should tag a document\n",
    "- schema: defines how we want to tag the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3020d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Quickstart\n",
    "Let's see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We'll use the with_structured_output method supported by OpenAI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fd1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading the .env file. Using full path to use it in the interactive shell\n",
    "load_dotenv(\"C:\\\\Users\\\\MarcusChiri\\\\Dropbox\\\\Repositories\\\\Langchain\\\\.env\")\n",
    "\n",
    "# Loarding the environment variables\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6884b6",
   "metadata": {},
   "source": [
    "Let's specify a Pydantic model with a few properties and their expected type in our schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the text\")\n",
    "    aggressiveness: int = Field(\n",
    "        description=\"How aggressive the text is on a scale from 1 to 10\"\n",
    "    )\n",
    "    language: str = Field(description=\"The language the text is written in\")\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "# Structured LLM\n",
    "structured_llm = llm.with_structured_output(Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ecb8ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification(sentiment='positive', aggressiveness=1, language='Spanish')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\n",
    "prompt = tagging_prompt.invoke({\"input\": inp})\n",
    "response = structured_llm.invoke(prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a5ed5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification(sentiment='passionate', aggressiveness=3, language='Portuguese')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Aqui tem um bando de loucos! Louco por ti Corinthians! Aqueles que acham que é pouco, eu vivo por ti Corinthians! Eu canto até ficar rouco! Eu canto pra te empurrar! Vamo vamo meu timão, vamo meu timão! Não para de lutar!\"\n",
    "prompt = tagging_prompt.invoke({\"input\": inp})\n",
    "response = structured_llm.invoke(prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe5827a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification(sentiment='negative', aggressiveness=8, language='Portuguese')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Se o Corinthians não ganhar, o pau vai quebrar\"\n",
    "prompt = tagging_prompt.invoke({\"input\": inp})\n",
    "response = structured_llm.invoke(prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae223a07",
   "metadata": {},
   "source": [
    "If we want dictionary output, we can just call .model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f129427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'angry', 'aggressiveness': 8, 'language': 'Spanish'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
    "prompt = tagging_prompt.invoke({\"input\": inp})\n",
    "response = structured_llm.invoke(prompt)\n",
    "\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce414303",
   "metadata": {},
   "source": [
    "As we can see in the examples, it correctly interprets what we want.\n",
    "\n",
    "The results vary so that we may get, for example, sentiments in different languages ('positive', 'enojado' etc.).\n",
    "\n",
    "We will see how to control these results in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027cd44",
   "metadata": {},
   "source": [
    "### Finer Control\n",
    "\n",
    "Finer control\n",
    "Careful schema definition gives us more control over the model's output.\n",
    "\n",
    "Specifically, we can define:\n",
    "\n",
    "Possible values for each property\n",
    "Description to make sure that the model understands the property\n",
    "Required properties to be returned\n",
    "Let's redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1dd9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n",
    "    aggressiveness: int = Field(\n",
    "        ...,\n",
    "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
    "        enum=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    language: str = Field(\n",
    "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "489f2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\").with_structured_output(\n",
    "    Classification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d838f",
   "metadata": {},
   "source": [
    "Now the answers will be restricted in a way we expect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3f485f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification(sentiment='happy', aggressiveness=1, language='spanish')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\n",
    "prompt = tagging_prompt.invoke({\"input\": inp})\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45b66388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification(sentiment='sad', aggressiveness=4, language='spanish')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
    "prompt = tagging_prompt.invoke({\"input\": inp})\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1865850",
   "metadata": {},
   "source": [
    "## Going deeper\n",
    "You can use the [metadata tagger](https://python.langchain.com/docs/integrations/document_transformers/openai_metadata_tagger) document transformer to extract metadata from a LangChain Document.\n",
    "This covers the same basic functionality as the tagging chain, only applied to a LangChain Document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain-iLGQdGpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
